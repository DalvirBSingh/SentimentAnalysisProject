{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# the following line allows ipython to display plots\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ds070111/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# read this csv file, remember to put the full path to \n",
    "# the directory where you saved the data\n",
    "df = pd.read_csv('1429_1.csv')  # df is DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function that we will use to transform the rating to sentiment value\n",
    "def label_sentiment (row, attribute):\n",
    "    if row[attribute] >3 :\n",
    "        return 1\n",
    "    if row[attribute] <3 :\n",
    "        return 3\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV, filter the columns for the desired columns, \n",
    "df = pd.read_csv('1429_1.csv')  # df is DataFrame object\n",
    "df['sentiment_label'] = df.apply (lambda row: label_sentiment(row, \"reviews.rating\"), axis=1)\n",
    "df = df.filter(['reviews.text', 'sentiment_label'])\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11795</th>\n",
       "      <td>gave as a gift along with the cover. person se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23104</th>\n",
       "      <td>Had it for about year and would say it had goo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>Not the fastest for loading web pages, but app...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16157</th>\n",
       "      <td>I bought this as a gift for my smart 1 yr old ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7485</th>\n",
       "      <td>I bought for my niece and she loves it. The fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews.text  sentiment_label\n",
       "11795  gave as a gift along with the cover. person se...                1\n",
       "23104  Had it for about year and would say it had goo...                1\n",
       "4824   Not the fastest for loading web pages, but app...                1\n",
       "16157  I bought this as a gift for my smart 1 yr old ...                1\n",
       "7485   I bought for my niece and she loves it. The fa...                1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-bffe6abf7db5>:32: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ds070111/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import the classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text Class which will clea dataframe column which consists of text\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    # removing punctations\n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "\n",
    "        return str(input_text).translate(trantab)\n",
    "    # Remove numbers\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    # Make it lowercase\n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    # Remove stop words\n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    # Stemming\n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    # Fit it\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    # Transform the column\n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will create a bag of words give dataframe column\n",
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    print ('Creating bag of words...')\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    # In this example features may be single words or two consecutive words\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000,\n",
    "                                max_df = 0.5,\n",
    "                                min_df =1) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings. The output is a sparse array\n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Convert to a NumPy array for easy of handling\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    # tfidf transform\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
    "\n",
    "    # Take a look at the words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    print ('Completed bag of words...')\n",
    "    print(' Returning vectorizer, vocab, train_data_features, tfidf_features, tfidf.....')\n",
    "    return vectorizer, vocab, train_data_features, tfidf_features, tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# train logistic regression\n",
    "def train_LG(X, Y):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    ml_model = LogisticRegression(C = 0.2,penalty= 'l2', random_state = 0)\n",
    "    ml_model.fit(X, Y)\n",
    "    print ('Completed!')\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train NBC classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def train_NB(X, Y):\n",
    "    print (\"Training Naive Bayes model...\")\n",
    "    ml_model = MultinomialNB()\n",
    "    ml_model.fit(X, Y)\n",
    "    print ('Completed!')\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def train_SVM(X, Y):\n",
    "    print (\"Training SVM...\")\n",
    "    ml_model = svm.SVC()\n",
    "    ml_model.fit(X, Y)\n",
    "    print ('Completed!')\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_y, y_test): \n",
    "    correctly_identified_y = predicted_y == y_test\n",
    "    accuracy = np.mean(correctly_identified_y) * 100\n",
    "    print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm(algorithm):\n",
    "    if(algorithm == \"LG\"):\n",
    "        # Get the model\n",
    "        ml_model = train_LG(tfidf_features, y_train)\n",
    "        # vectorize the attributes of the test data after it has been cleaned\n",
    "        test_data_features = vectorizer.transform(X_test['clean_text'])\n",
    "        # TF\n",
    "        test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "        #Predict \n",
    "        predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "        #Accuracy\n",
    "        accuracy(predicted_y, y_test)\n",
    "        return ml_model\n",
    "    elif(algorithm ==\"NB\"):\n",
    "        # Get the model\n",
    "        ml_model = train_NB(tfidf_features, y_train)\n",
    "        # vectorize the attributes of the test data after it has been cleaned\n",
    "        test_data_features = vectorizer.transform(X_test['clean_text'])\n",
    "        # TF\n",
    "        test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "        #Predict \n",
    "        predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "        #Accuracy\n",
    "        accuracy(predicted_y, y_test)\n",
    "        return ml_model\n",
    "    else:\n",
    "        # Get the model\n",
    "        ml_model = train_SVM(tfidf_features, y_train)\n",
    "        # vectorize the attributes of the test data after it has been cleaned\n",
    "        test_data_features = vectorizer.transform(X_test['clean_text'])\n",
    "        # TF\n",
    "        test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "        #Predict \n",
    "        predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "        #Accuracy\n",
    "        accuracy(predicted_y, y_test)\n",
    "        return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bag of words...\n",
      "Completed bag of words...\n",
      " Returning vectorizer, vocab, train_data_features, tfidf_features, tfidf.....\n"
     ]
    }
   ],
   "source": [
    "# Main Driver\n",
    "\n",
    "# Clean the text column\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(df['reviews.text'])\n",
    "\n",
    "# Create clean_text column\n",
    "df_model = df\n",
    "df_model['clean_text'] = sr_clean\n",
    "df_model.columns.tolist()\n",
    "\n",
    "# Split the data(cleaned, preprocessed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model.drop('sentiment_label', axis=1), df_model.sentiment_label, test_size=0.45, random_state=2)\n",
    "\n",
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf  = (\n",
    "        create_bag_of_words(X_train['clean_text']))\n",
    "\n",
    "vectorizer1 = vectorizer\n",
    "tfidf1 = tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Logistic Regression....\n",
      "Training the logistic regression model...\n",
      "Completed!\n",
      "93.23587869462075\n",
      "Testing Naive Bayes....\n",
      "Training Naive Bayes model...\n",
      "Completed!\n",
      "93.21664422645381\n",
      "Testing SVM....\n",
      "Temporarily Skipping SVM....\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Logistic Regression....\")\n",
    "lg_model_1 = test_algorithm(\"LG\")\n",
    "print(\"Testing Naive Bayes....\")\n",
    "nb_model_1 = test_algorithm(\"NB\")\n",
    "print(\"Testing SVM....\")\n",
    "print(\"Temporarily Skipping SVM....\")\n",
    "#test_algorithm(\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tfidf_1.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump predictive models for dataset 1\n",
    "joblib.dump(lg_model_1, \"./lg_model_1.joblib\", compress=True)\n",
    "joblib.dump(nb_model_1, \"./nb_model_1.joblib\", compress=True)\n",
    "joblib.dump(vectorizer1, \"./vectorizer_1.joblib\", compress=True)\n",
    "joblib.dump(tfidf1, \"./tfidf_1.joblib\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon DataSet... \n",
      "Cleaning DataSet...\n",
      "Splitting Dataset....\n",
      "Creating bag of words...\n",
      "Completed bag of words...\n",
      " Returning vectorizer, vocab, train_data_features, tfidf_features, tfidf.....\n",
      "Testing Logistic Regression....\n",
      "Training the logistic regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ds070111/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n",
      "87.4589897177437\n",
      "Testing Naive Bayes....\n",
      "Temporarily Skippung NB....\n",
      "Testing SVM....\n",
      "Temporarily Skipping SVM....\n"
     ]
    }
   ],
   "source": [
    "# Second Driver Program \n",
    "\n",
    "#Load New Data\n",
    "print(\"Loading Amazon DataSet... \")\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df['sentiment_label'] = df.apply (lambda row: label_sentiment(row, \"Score\"), axis=1)\n",
    "df = df.filter(['Text', 'sentiment_label'])\n",
    "\n",
    "print(\"Cleaning DataSet...\")\n",
    "#Clean the text column\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(df['Text'])\n",
    "\n",
    "# Create clean_text column\n",
    "df_model = df\n",
    "df_model['clean_text'] = sr_clean\n",
    "df_model.columns.tolist()\n",
    "\n",
    "print(\"Splitting Dataset....\")\n",
    "# Split the data(cleaned, preprocessed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model.drop('sentiment_label', axis=1), df_model.sentiment_label, test_size=0.20, random_state=2)\n",
    "\n",
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf  = (\n",
    "        create_bag_of_words(X_train['clean_text']))\n",
    "\n",
    "vectorizer2 = vectorizer\n",
    "tfidf2 = tfidf\n",
    "\n",
    "print(\"Testing Logistic Regression....\")\n",
    "lg_model_2 = test_algorithm(\"LG\")\n",
    "print(\"Testing Naive Bayes....\")\n",
    "print(\"Temporarily Skippung NB....\")\n",
    "#test_algorithm(\"NB\")\n",
    "print(\"Testing SVM....\")\n",
    "print(\"Temporarily Skipping SVM....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tfidf2.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dump the predictive models for data set 2\n",
    "joblib.dump(lg_model_2, \"./lg_model2.joblib\", compress=True)\n",
    "joblib.dump(vectorizer2, \"./vectorizer2.joblib\", compress=True)\n",
    "joblib.dump(tfidf2, \"./tfidf2.joblib\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\"\"\"Yuk and yuk! It tastes like something from dollar store. I like a cup of coffe, just a plain cup of coffee and this is brown water. For all you Massachusetts people, this is nothing like the dunkin coffee you get. This is packaged dunkin but it should be generic coffee. I’m sure Folgers is better\"\"\"]\n",
    "\n",
    "# user_data_df = pd.DataFrame(data, columns = ['text']) \n",
    "        \n",
    "# user_data_features = vectorizer2.transform(user_data_df['text'])\n",
    "\n",
    "# user_data_tfidf_features = tfidf2.fit_transform(user_data_features)\n",
    "\n",
    "# lg_model_2.predict(user_data_tfidf_features)[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
